(base) dutxutengfei@dutxutengfei:~/vae-gan$ conda activate tensorflowpython37
(tensorflowpython37) dutxutengfei@dutxutengfei:~/vae-gan$ python vaer_gan_z14.py
2021-10-30 12:04:18.769633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:18.773201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:18.773876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:18.774709: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-30 12:04:18.775560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:18.776286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:18.776969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:19.017929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:19.018618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:19.019282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:19.019912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22058 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6
1
2021-10-30 12:04:19.307841: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/100
2021-10-30 12:04:22.373093: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201
2021-10-30 12:04:23.111086: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2021-10-30 12:04:23.285634: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
    466/Unknown - 14s 20ms/step - vae_loss: 40610.4297 - kl_loss: 37392.4922 - reconstruction_loss: 1450.4314 - discriminator_loss: 3.2186 - refine_loss: 1665.4252^CTraceback (most recent call last):
  File "vaer_gan_z14.py", line 609, in <module>
    VAER_GAN_instance, a128batch), MyepochsaveCallback(save_dir, VAER_GAN_instance)])
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/keras/engine/training.py", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 885, in __call__
    result = self._call(*args, **kwds)
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 917, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 3040, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1964, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 596, in call
    ctx=ctx)
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
KeyboardInterrupt
(tensorflowpython37) dutxutengfei@dutxutengfei:~/vae-gan$ python vaer_gan_z14.py
2021-10-30 12:04:41.761425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:41.765006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:41.765681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:41.766484: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-30 12:04:41.767238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:41.767928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:41.768602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:42.004306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:42.004978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:42.005605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-30 12:04:42.006238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22051 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6
1
2021-10-30 12:04:42.295376: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/100
2021-10-30 12:04:45.303708: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201
2021-10-30 12:04:45.776900: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2021-10-30 12:04:46.156798: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2810/2810 [==============================] - 62s 20ms/step - vae_loss: 1716.4987 - kl_loss: 54.7431 - reconstruction_loss: 780.2336 - discriminator_loss: 0.5052 - refine_loss: 787.1024
Epoch 2/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 819.0396 - kl_loss: 45.5111 - reconstruction_loss: 340.0821 - discriminator_loss: 0.4472 - refine_loss: 368.3414
Epoch 3/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 618.5303 - kl_loss: 40.9676 - reconstruction_loss: 242.3847 - discriminator_loss: 0.3966 - refine_loss: 268.5704
Epoch 4/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 539.4080 - kl_loss: 39.9219 - reconstruction_loss: 196.6528 - discriminator_loss: 0.3940 - refine_loss: 219.3425
Epoch 5/100
2810/2810 [==============================] - 57s 20ms/step - vae_loss: 471.3134 - kl_loss: 39.5688 - reconstruction_loss: 163.9332 - discriminator_loss: 0.4112 - refine_loss: 185.3670
Epoch 6/100
2810/2810 [==============================] - 58s 21ms/step - vae_loss: 416.6245 - kl_loss: 40.3751 - reconstruction_loss: 136.9779 - discriminator_loss: 0.3728 - refine_loss: 158.1712
Epoch 7/100
2810/2810 [==============================] - 58s 21ms/step - vae_loss: 366.5137 - kl_loss: 40.6117 - reconstruction_loss: 116.2176 - discriminator_loss: 0.3598 - refine_loss: 135.2485
Epoch 8/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 334.6915 - kl_loss: 40.9596 - reconstruction_loss: 101.4545 - discriminator_loss: 0.3697 - refine_loss: 120.5142
Epoch 9/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 318.1283 - kl_loss: 41.5339 - reconstruction_loss: 91.9518 - discriminator_loss: 0.3546 - refine_loss: 109.3480
Epoch 10/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 305.8386 - kl_loss: 42.4320 - reconstruction_loss: 85.8786 - discriminator_loss: 0.3586 - refine_loss: 100.3897
Epoch 11/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 295.8948 - kl_loss: 42.4245 - reconstruction_loss: 80.8673 - discriminator_loss: 0.3217 - refine_loss: 94.5923
Epoch 12/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 293.8159 - kl_loss: 42.7531 - reconstruction_loss: 77.3258 - discriminator_loss: 0.2248 - refine_loss: 90.1149
Epoch 13/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 311.4152 - kl_loss: 43.9417 - reconstruction_loss: 75.8000 - discriminator_loss: 0.1908 - refine_loss: 85.6847
Epoch 14/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 281.1879 - kl_loss: 43.6102 - reconstruction_loss: 69.8700 - discriminator_loss: 0.2482 - refine_loss: 81.8810
Epoch 15/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 266.1912 - kl_loss: 43.2437 - reconstruction_loss: 66.6370 - discriminator_loss: 0.2469 - refine_loss: 78.0940
Epoch 16/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 261.7806 - kl_loss: 42.8773 - reconstruction_loss: 62.6897 - discriminator_loss: 0.2998 - refine_loss: 74.4289
Epoch 17/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 267.7607 - kl_loss: 43.0126 - reconstruction_loss: 62.7996 - discriminator_loss: 0.2336 - refine_loss: 74.0041
Epoch 18/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 284.4916 - kl_loss: 43.4599 - reconstruction_loss: 62.5864 - discriminator_loss: 0.1176 - refine_loss: 72.1714
Epoch 19/100
2810/2810 [==============================] - 56s 20ms/step - vae_loss: 321.8476 - kl_loss: 46.0379 - reconstruction_loss: 64.5278 - discriminator_loss: 0.1442 - refine_loss: 73.8265
Epoch 20/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 338.1624 - kl_loss: 47.1262 - reconstruction_loss: 64.6187 - discriminator_loss: 0.1156 - refine_loss: 73.8859
Epoch 21/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 323.4073 - kl_loss: 49.1552 - reconstruction_loss: 63.9561 - discriminator_loss: 0.2280 - refine_loss: 70.9746
Epoch 22/100
2810/2810 [==============================] - 55s 20ms/step - vae_loss: 348.3289 - kl_loss: 51.0108 - reconstruction_loss: 62.8497 - discriminator_loss: 0.2824 - refine_loss: 72.1897
Epoch 23/100
1441/2810 [==============>...............] - ETA: 27s - vae_loss: 311.7495 - kl_loss: 51.8089 - reconstruction_loss: 60.9921 - discriminator_loss: 0.2358 - refine_loss: 70.8671Traceback (most recent call last):
  File "vaer_gan_z14.py", line 609, in <module>
    VAER_GAN_instance, a128batch), MyepochsaveCallback(save_dir, VAER_GAN_instance)])
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/keras/engine/training.py", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 885, in __call__
    result = self._call(*args, **kwds)
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 917, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 3040, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1964, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 596, in call
    ctx=ctx)
  File "/home/dutxutengfei/anaconda3/envs/tensorflowpython37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) Data loss:  corrupted record at 98480
	 [[node IteratorGetNext (defined at vaer_gan_z14.py:609) ]]
  (1) Data loss:  corrupted record at 98480
	 [[node IteratorGetNext (defined at vaer_gan_z14.py:609) ]]
	 [[IteratorGetNext/_2]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_9290]

Function call stack:
train_function -> train_function

